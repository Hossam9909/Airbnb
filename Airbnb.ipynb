{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# 1. Load Data Function\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a DataFrame with error handling.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded data from {file_path}\\n\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: The file {file_path} was not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        raise ValueError(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        raise ValueError(\"Error: The file could not be parsed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "def display_data_info(df):\n",
    "    \"\"\"\n",
    "    Display detailed information about the DataFrame, including:\n",
    "    - Number of rows and columns.\n",
    "    - Number of columns with missing data.\n",
    "    - Data type of each column.\n",
    "    - Summary statistics for numerical and categorical data.\n",
    "    - Percentage of missing values per column.\n",
    "    - Number of duplicate rows.\n",
    "    - Number of unique values per column.\n",
    "    - Correlation matrix for numerical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing the dataset...\\n\")\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "    # Missing Data Information\n",
    "    missing_counts = df.isna().sum()\n",
    "    num_cols_missing = (missing_counts > 0).sum()\n",
    "    print(f\"Number of columns with missing data: {num_cols_missing}\")\n",
    "\n",
    "    if num_cols_missing > 0:  # Only show details if there are any missing values.\n",
    "        missing_percentage = (missing_counts / len(df)) * 100\n",
    "        missing_info = pd.DataFrame({\n",
    "            'Missing Count': missing_counts,\n",
    "            'Missing Percentage': missing_percentage\n",
    "        }).round(2)\n",
    "        print(\"\\nMissing Value Information:\")\n",
    "        print(tabulate(missing_info, headers='keys', tablefmt='pretty', showindex=True))  #Show index (column names)\n",
    "\n",
    "\n",
    "    # Data Type Information\n",
    "    print(\"\\nData types of each column:\")\n",
    "    print(tabulate(df.dtypes.reset_index(), headers=['Column', 'Data Type'], tablefmt='pretty'))\n",
    "\n",
    "    # Summary Statistics (Numerical)\n",
    "    print(\"\\nSummary statistics for numerical data:\")\n",
    "    numerical_summary = df.describe(include=np.number).round(2)\n",
    "    print(tabulate(numerical_summary, headers='keys', tablefmt='pretty'))\n",
    "\n",
    "\n",
    "    # Summary Statistics (Categorical) - Improved\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns #Handles both object and category dtypes\n",
    "    if categorical_cols.size > 0:\n",
    "        print(\"\\nSummary statistics for categorical data:\")\n",
    "        categorical_summary = df[categorical_cols].describe().T\n",
    "        categorical_summary['Distinct Count'] = df[categorical_cols].nunique() #Adds distinct count, handles missing values better.\n",
    "        print(tabulate(categorical_summary, headers='keys', tablefmt='pretty'))\n",
    "\n",
    "    # Unique Values\n",
    "    print(\"\\nNumber of unique values per column:\")\n",
    "    print(tabulate(df.nunique().reset_index(), headers=['Column', 'Unique Values'], tablefmt='pretty'))\n",
    "\n",
    "\n",
    "    # Correlation Matrix (Numerical)\n",
    "    print(\"\\nCorrelation matrix for numerical columns:\")\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = numeric_df.corr().round(2)\n",
    "    print(tabulate(correlation_matrix, headers='keys', tablefmt='psql'))  # 'psql' is visually nicer for matrices\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Clean Data Function\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the dataset by removing empty columns and preprocessing 'last_review'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Cleaned DataFrame with:\n",
    "        - Columns with 100% missing values removed.\n",
    "        - 'last_review' column converted to datetime and missing values imputed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove columns with all missing values\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # Convert 'last_review' to datetime and fill missing with placeholder\n",
    "    df.loc[:, 'last_review'] = pd.to_datetime(df['last_review'], errors='coerce').dt.date\n",
    "    df.loc[:, 'last_review'] = df['last_review'].fillna('No reviews')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Impute Missing Values\n",
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values for numerical columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with imputed values.\n",
    "    \"\"\"\n",
    "    # Impute 'reviews_per_month' and 'price' with the median\n",
    "    \n",
    "    # Rationale: The median is robust to outliers and provides a central value for imputation.\n",
    "    df['reviews_per_month'] = df['reviews_per_month'].fillna(df['reviews_per_month'].median())\n",
    "    \n",
    "    # Rationale: Using the median for price ensures outlier influence is minimized.\n",
    "    df['price'] = df['price'].fillna(df['price'].median())\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
